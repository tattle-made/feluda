{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3725096f",
   "metadata": {},
   "source": [
    "# Detect Lewd Content in Images with Feluda\n",
    "\n",
    "This notebook demonstrates how to use the `DetectLewdImages` operator to analyze\n",
    "images for inappropriate content. It processes a sample image and displays the\n",
    "probability score indicating likelihood of lewd content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38455989",
   "metadata": {},
   "source": [
    "[![GitHub](https://img.shields.io/badge/GitHub-View%20Source-blue?logo=github)](https://github.com/tattle-made/feluda/blob/main/docs/examples/detect_lewd_images.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tattle-made/feluda/blob/main/docs/examples/detect_lewd_images.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d79e1",
   "metadata": {},
   "source": [
    "Install dependencies conditionally based on whether the notebook is running in Colab or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d892e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Notebook locally\n",
      "\u001b[2mUsing Python 3.10.12 environment at: /home/aatman/Aatman/Tattle/feluda/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n",
      "CPU times: user 6.38 ms, sys: 4.13 ms, total: 10.5 ms\n",
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(\"Running Notebook in Google Colab\" if IN_COLAB else \"Running Notebook locally\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Since Google Colab has preinstalled libraries like tensorflow and numba, we create a folder called feluda_custom_venv and isolate the environment there.\n",
    "    # This is done to avoid any conflicts with the preinstalled libraries.\n",
    "    %pip install uv\n",
    "    !mkdir -p /content/feluda_custom_venv\n",
    "    !uv pip install --target=/content/feluda_custom_venv --prerelease allow feluda feluda-detect-lewd-images > /dev/null 2>&1\n",
    "\n",
    "    sys.path.insert(0, \"/content/feluda_custom_venv\")\n",
    "else:\n",
    "    !uv pip install feluda feluda-detect_lewd-images > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a99415-ca33-4395-8bf0-84b109bcd382",
   "metadata": {},
   "source": [
    "We'll use one operator for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143fc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23195e656ce427cb7e29e1258de042f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from feluda.factory import ImageFactory\n",
    "from feluda.operators import DetectLewdImages\n",
    "\n",
    "detector = DetectLewdImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d10961-4a9f-4027-acfc-a280de6a0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image from URL\n",
      "\n",
      "Image downloaded\n"
     ]
    }
   ],
   "source": [
    "IMAGE_URL = \"https://github.com/tattle-made/feluda_datasets/blob/main/feluda-sample-media/people.jpg\"\n",
    "\n",
    "raw_url = IMAGE_URL.replace(\"/blob/\", \"/raw/\")\n",
    "\n",
    "# Download image using ImageFactory\n",
    "image_obj = ImageFactory.make_from_url_to_path(raw_url)\n",
    "\n",
    "# Analyze image for inappropriate content\n",
    "# Returns probability score (0.0 to 1.0) indicating likelihood of lewd content\n",
    "probability = detector.run(image_obj, remove_after_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92ecda-1b9b-40f2-b159-42e8b68520d9",
   "metadata": {},
   "source": [
    "In the below codeblock, we are predicting the probability of lewd content present in an image using the `feluda-detect-lewd-images` operator. The operator uses the Private Detector model from Bumble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad07d1a7-5e74-4604-aab4-6f53bca59dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lewd content probability: 0.0543\n",
      "Result: SAFE - Low probability of inappropriate content\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lewd content probability: {probability:.4f}\")\n",
    "\n",
    "# Interpret the result\n",
    "if probability < 0.3:\n",
    "    print(\"Result: SAFE - Low probability of inappropriate content\")\n",
    "elif probability < 0.7:\n",
    "    print(\"Result: MODERATE - Medium probability of inappropriate content\")\n",
    "else:\n",
    "    print(\"Result: UNSAFE - High probability of inappropriate content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a50a4a-a1f9-415f-9bfe-780c9427cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources when you're done\n",
    "\n",
    "detector.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
